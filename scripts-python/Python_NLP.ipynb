{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# !pip install gensim\n",
    "# !pip install nltk\n",
    "# !pip install sklearn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import itertools\n",
    "import numpy as np\n",
    "import re        \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "pd.options.display.max_colwidth = 100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "base = pd.read_csv(\"../dados/nlp/baselimpa.csv\")\n",
    "# Not included in the repo due to privacy concerns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "base[base['tweet'].str.contains(\"escola\")]['tweet']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7                  estudantes da escola municipal do complexo da maré agachados sob ataque do caveirão aéreo\n",
       "10       #repost @bertjoi with get_repost ・・・ agatha. 8 anos. tiro nas costas. estava dentro da kombi/lot...\n",
       "12       #witzelassassino.  parem de nos matar!  abaixo criançasda escolamunicipal no pinheiro, complexo ...\n",
       "13       nas escolas que foram alvos do caveirão aéreo, funcionários ainda estão com medo. a maioria se m...\n",
       "17       nós, moradores minimamente alertamos em dias de operações policiais: atividade morador, não leve...\n",
       "                                                        ...                                                 \n",
       "10347    aguia de ouro tentou dar uma lacradinha no ensaio técnico dessa semana (sexta se não me engano) ...\n",
       "10348    a crítica política da águia de ouro é mais um tiro no pé dessa escola de samba que logo logo vai...\n",
       "10463                                                        eu dá escola hoje vi os cana joga tiro do águia\n",
       "10583    águia deu tiro p dentro da casa da minha tia pq um fdp invadiu lá p se esconder, mano e se pega ...\n",
       "10643    essas porra ficam falando que o povo de favela não estuda porque não quer pipipopo   sendo que a...\n",
       "Name: tweet, Length: 564, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "def tokenization_s(sentences): \n",
    "    s_new = []\n",
    "    for sent in (sentences[:][0]): \n",
    "        s_token = sent_tokenize(sent)\n",
    "        if s_token != '':\n",
    "            s_new.append(s_token)\n",
    "    return s_new"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "def preprocess(text):\n",
    "    clean_data = []\n",
    "    for x in (text[:][0]): #this is Df_pd for Df_np (text[:])\n",
    "       # new_text = re.sub('<.*?>', '', x)   # remove HTML tags\n",
    "        new_text = re.sub(r'[^\\w\\s]', '', x) # remove punc.\n",
    "       # new_text = re.sub(r'\\d+','',new_text)# remove numbers\n",
    "        new_text = new_text.lower() # lower case, .upper() for upper          \n",
    "        if new_text != '':\n",
    "            clean_data.append(new_text)\n",
    "    return clean_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "def tokenization_w(words):\n",
    "    w_new = []\n",
    "    for w in (words[:][0]):  # for NumPy = words[:]\n",
    "        w_token = word_tokenize(w)\n",
    "        if w_token != '':\n",
    "            w_new.append(w_token)\n",
    "    return w_new"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "snowball = SnowballStemmer(language = 'portuguese')\n",
    "def stemming(words):\n",
    "    new = []\n",
    "    stem_words = [snowball.stem(x) for x in (words[:][0])]\n",
    "    new.append(stem_words)\n",
    "    return new"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "msgs = base['tweet'].astype(str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "#nltk.word_tokenize(base.tweet)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "stopwords_en = nltk.corpus.stopwords.words('english')\n",
    "# Define palavras a serem excluídas\n",
    "newStopWords = ['lula','http','3r6nsl','u200d','helicoptero','policia',\n",
    "                'aeronave','ly','tb','tche', 'pra', '…','cv43sn','gate21','xa0','tinyurl',\n",
    "                'helicóptero','durante','aguia','bit','https','fu','ahh','br','tiro',\n",
    "                'bit','favela','sido','cara','cv43sn\\'','enquanto', 'pic', 'twitter', 'www', 'ser'\n",
    "               'ver','suzano','tipo','xa0\\'','bom','bolivia','anti','diz','xa0helicoptero','apos','dois',\n",
    "                'list','rio','da','em','quem','por','na','mas','ja','era','para','mais','se','nao','do',\n",
    "                'que','de','ou','com','teria','sempre','outro','ao','os','duas','at','uma','um',\n",
    "                'tiros','tiro','disparo','helicoptero','q','ta','pq','html','01','2011','tá','ig_twitter_share','uns']\n",
    "\n",
    "# STOPWORDS = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "stopwords.extend(newStopWords)\n",
    "stopwords.extend(stopwords_en)\n",
    "len(stopwords)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "469"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "# stopwords"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# Topic model w/ LDA\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words= stopwords)\n",
    "doc_term_matrix = tfidf_vect.fit_transform(base.tweet.values.astype('U'))\n",
    "\n",
    "nmf = NMF(n_components=3, random_state=42)\n",
    "nmf.fit(doc_term_matrix )\n",
    "\n",
    "for i,topic in enumerate(nmf.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-30:]])\n",
    "    print('\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/abitporu/.pyenv/versions/3.9.0/envs/lede/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top 10 words for topic #0:\n",
      "['proteja', 'alto', 'dia', 'cana', 'porra', 'gente', 'voando', 'tava', 'tão', 'dessa', 'morro', 'mané', 'hora', 'nada', 'acordei', 'caralho', 'baixinho', 'mano', 'rasante', 'crlh', 'passou', 'mt', 'vários', 'deus', 'crl', 'passando', 'cima', 'baixo', 'águia', 'dando']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['moradores', 'tudo', 'dentro', 'ter', 'rj', 'atirando', 'manhã', 'perto', 'deu', 'civil', 'complexo', 'rua', 'pm', 'hoje', 'nada', 'governador', 'lá', 'ser', 'gente', 'escola', 'agora', 'status', 'cima', 'dar', 'operação', 'sobrevoando', 'vai', 'casa', 'polícia', 'aqui']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['ouvir', 'hj', 'acordada', 'hoje', 'agr', 'águia', 'acordo', 'manhã', 'ouvindo', 'escutei', 'sobrevoando', 'nada', 'ouvi', 'dormir', 'susto', 'despertador', 'acorda', 'acordou', 'bala', 'morador', 'acerta', 'som', 'perdida', 'terror', 'botando', 'chega', 'acordar', 'dia', 'acordei', 'barulho']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA.fit(doc_term_matrix)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=42)"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words= stopwords)\n",
    "# doc_term_matrix = count_vect.fit_ransform(base.tweet.values.astype('U'))\n",
    "\n",
    "# doc_term_matrix"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'fit_ransform'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_190112/1998338779.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdoc_term_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_ransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'U'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'fit_ransform'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# Remove palavras comuns do dicionário padrão\n",
    "tweets = [[word for word in tweet_words if not word in stopwords]\n",
    "              for tweet_words in msgs]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "words_in_tweet = [tweet.lower().split() for tweet in base.tweet]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# words_in_tweet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Conta palavras mais frequentes\n",
    "#palavras = list(itertools.chain(*base.tweet))\n",
    "\n",
    "#counts_no_urls = collections.Counter(palavras)\n",
    "\n",
    "#counts_no_urls.most_common(200)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Mostra as mensagens mais retweetadas\n",
    "#pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "#base.nlargest(50, columns='retweets_count')[['date','tweet', 'username','retweets_count']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#corpus = [dictionary.doc2bow(mensagens) for text in mensagens]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('lede': pyenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "interpreter": {
   "hash": "729fdc33958623437e6ce46216c742c17eeee0026f81981456a74df7524a9356"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}